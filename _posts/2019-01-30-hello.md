---
title: January thoughts, Future of AI, Isaiah Berlin and Brainspotting 
updated: 2019-01-30 21:32
---
Hello,
New year, new start, new goals. Hopefully, everyone makes peace with their resolutions. It is a hard job to keep new habits on regular basis, so sometime a break is okay. I just want to remind that you can always start from zero tomorrow morning. 
About my resolutions, blogging is one of them. I enjoy going from one tab to another when I procrastinate. Between  these tabs I find some valuable thoughts to share.Let’s see together where it goes. For the first post, there are a few things I want to mention this month:

In Japan it is a tradition to end the year with a [Bōnenkai party](https://en.wikipedia.org/wiki/B%C5%8Dnenkai). Our [Machine Learning Tokyo](https://machinelearningtokyo.com/) Bonenkai party was very thoughtful and fun at the same time. Since we are ML scientists/engineers it is inevitable to discuss what is the future impact of AI/ML in our daily lives. I could not stop myself thinking these questions and try to find some answers. 

![Stanford AI 100](https://engineering.stanford.edu/sites/default/files/styles/banner-850x400/public/ai100_robot-concept_1__0.jpg?itok=0kdQhP8E)

Luckily, I don't have to solely trust my imagination while thinking the future. Stanford has published [100 years of AI](https://ai100.stanford.edu/) report about the past, now and future of intelligence. As they mentioned in the very beginning: 
> "Unlike in the movies, there is no race of superhuman robots on the horizon or probably even possible."

This gave me a quick relaxation and another question : “WHY are we not there yet?”  

Will one day a robot take my job? Most probably yes; however, in many realms, AI will likely replace tasks rather than jobs in the near term, and will also create new kinds of jobs. And it will create new areas that we cannot foresee today. When it comes to future economy model, AI may provide us another mechanism for wealth. Maybe, we will be more concerned about our portion of the world’s AI power rather than stocks.

As the panel suggests transportation is likely to be one of the first domains (sorry but no flying with quadcopters in 2030) in which the general public will be asked to trust the reliability and safety of an AI system for a critical task. When it comes to humans, we are just given a 15 min driving test and we are entitled to drive until we die. However, when it comes to self-driving cars, we are more skeptical to trust them than we trust a foreign Uber driver. How do we ensure that a machine is capable of doing things? Do we need to test for each use case? Is it even possible to create simulations of every driving experience?  Such questions also asked by [Stuart Russell](https://people.eecs.berkeley.edu/~russell/) in the [conversation with Lex Fridman](https://lexfridman.com/stuart-russell/). Trust issues with AI are not limited to transportation. AI-based applications could improve health outcomes and quality of life for millions of people in the futures—but only if they gain the trust of doctors, nurses, and patients. I always have a belief that the future will be a collaboration with humans and machines, and the report claims the same. Let’s prepare ourselves for the best. 

I am not a podcast person, but taking time everyday to think about the future of what I am working on, makes me take my code more seriously.  Also, in each conversation, you can find a different perspective on today and the future of AI. [As Josh Tenenbaum suggests:](https://www.youtube.com/watch?v=Pwm6DqdC4pU) 
> "Machines are successful with pattern recognition but they lack the perception of humans."

[Andrej Karpathy's blog post](https://karpathy.github.io/2012/10/22/state-of-computer-vision/) about where we are with the intelligence 7 years ago, is still valid and agrees Tenenbaum’s ideas. 

It is inevitable to question the difference between the human brain and the machine intelligence when we try to understand _WHY_ we are not there yet. We are aware of our surroundings and ourselves in the same time. We have _self-consciousness_, but what is it? Can it be consciousness without a self? Can computers have a self? The answers depend on how you define _self_: Some people think we cannot talk about self without a body. Some think it is in the brain or memories. Also, it makes us define a _soul_ when we think about our inner self.  If you want to think beyond these questions [Kim Campbell on Brainspotting](https://www.youtube.com/watch?v=ONrhqPfSiRM) has a series that sure keeps your mind busy.

We are not at the dystopian world that AI robots had invaded as science fiction movies foresee. We are now trying to get answers to our everyday questions with the machines. But how do we define a question? Is it something we need to know? Is it something that we need? How do computers find answers to our questions? Actually it depends with defining a question. As [Tomaso Poggio suggests](https://cbmm.mit.edu/video/mit-ai-brains-minds-and-machines-tomaso-poggio-lex-fridman) computers understand the questions with decomposition. While I was looking for the definition of “question”, I ended up with Isaiah Berlin. He categorizes questions in three types: There is one type of ordinary questions whose answers lie in our world. Common sense, empirical or scientific questions lie in this category. There might be some unanswered questions because we are not there in science yet; however, answers can be found by examining or making experiments. Second, there are formal questions. Questions of this kind cannot be answered by observing the outside world. Such as mathematics or game rules. The rules of the chess game is defined beforehand. When a player asks : “How does my horse move”, we do not make experiments, we just tell them “L-shape” like it is a unchangeable fact. The last category is the philosophical questions whose answers can be given neither by observation nor by defined rules. This is why we need philosophers as Berlin says, 
> "If we don't go further with these questions the society is ossified and becomes a dogma."

Occasionally questions change states, such as “Are we alone in the universe?” used to be a philosophical question. Now it is a scientific question that we can conduct experiments and learn the answer. How about  “Who is responsible when a self-driven car crashes or an intelligent medical device fails?”  Is it a logical or a scientific question? Or maybe a philosophical?

![Isaiah Berlin](http://berlin.wolf.ox.ac.uk/image_library/photos/photos_of_ib/bardabig.jpg)
Isaiah Berlin, who was a famous philosopher, author, multicultural, scholar and a historian of ideas talks about why do we need philosophy in  [his interview with Bryan Magee](https://www.youtube.com/watch?v=vib2rqJKS08). It is a delightful conversation with two brilliant people. I am sure you will become addicted to Berlin’s thick accent and his ideas. As he mentioned many of us avoid thinking deep questions. 
> “People don't want to dig up because people don't want their assumptions to be examined over well. We feel uncomfortable when we look into what our believes rest on. We are both frightened and curious about these things. Philosophers tell people what are they believing and why.” 

 It is true that Berlin was an [intellectual that we miss in 2019](https://www.bbc.co.uk/programmes/b09cvrmf). Today, there is no one big media such as radio or television that attracts all the attention. On one hand it is so easy to share your ideas and find like-minded people, yet on the other hand people spread over many new kinds of media that it is even harder for us to follow contemporary thinking minds over these media crumble. I even had to choose where to start this blog over many media. While January was a thoughtful month for me I want to share some other aspects of the month [in my discoveries post](http://blog.zehrah.net/january).

